{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN with sequence length 10\n",
      "Epoch: 0 Loss: 3.3659 Val Acc: 0.1111\n",
      "Epoch: 100 Loss: 1.6689 Val Acc: 0.3816\n",
      "Epoch: 200 Loss: 1.4321 Val Acc: 0.3962\n",
      "Epoch: 300 Loss: 1.9719 Val Acc: 0.3543\n",
      "Epoch: 400 Loss: 1.1042 Val Acc: 0.4130\n",
      "Epoch: 500 Loss: 1.4490 Val Acc: 0.4340\n",
      "Epoch: 600 Loss: 1.2345 Val Acc: 0.4465\n",
      "Epoch: 700 Loss: 1.2049 Val Acc: 0.4319\n",
      "Epoch: 800 Loss: 1.0078 Val Acc: 0.4549\n",
      "Epoch: 900 Loss: 1.3177 Val Acc: 0.4423\n",
      "Training Time: 41.446280002593994 seconds\n",
      "\n",
      "Training LSTM with sequence length 10\n",
      "Epoch: 0 Loss: 3.2722 Val Acc: 0.1426\n",
      "Epoch: 100 Loss: 0.0383 Val Acc: 0.4130\n",
      "Epoch: 200 Loss: 0.0917 Val Acc: 0.4675\n",
      "Epoch: 300 Loss: 0.0359 Val Acc: 0.4340\n",
      "Epoch: 400 Loss: 0.0367 Val Acc: 0.4696\n",
      "Epoch: 500 Loss: 0.0340 Val Acc: 0.4298\n",
      "Epoch: 600 Loss: 0.0365 Val Acc: 0.4528\n",
      "Epoch: 700 Loss: 0.0336 Val Acc: 0.4696\n",
      "Epoch: 800 Loss: 0.0364 Val Acc: 0.4780\n",
      "Epoch: 900 Loss: 0.0368 Val Acc: 0.4864\n",
      "Training Time: 42.56746196746826 seconds\n",
      "\n",
      "Training GRU with sequence length 10\n",
      "Epoch: 0 Loss: 3.1956 Val Acc: 0.1887\n",
      "Epoch: 100 Loss: 0.0477 Val Acc: 0.4696\n",
      "Epoch: 200 Loss: 0.0527 Val Acc: 0.4528\n",
      "Epoch: 300 Loss: 0.5879 Val Acc: 0.3962\n",
      "Epoch: 400 Loss: 0.3200 Val Acc: 0.3983\n",
      "Epoch: 500 Loss: 0.0858 Val Acc: 0.4067\n",
      "Epoch: 600 Loss: 0.0560 Val Acc: 0.4235\n",
      "Epoch: 700 Loss: 0.0627 Val Acc: 0.3878\n",
      "Epoch: 800 Loss: 0.4873 Val Acc: 0.3983\n",
      "Epoch: 900 Loss: 0.3941 Val Acc: 0.4046\n",
      "Training Time: 42.47338509559631 seconds\n",
      "\n",
      "Training RNN with sequence length 20\n",
      "Epoch: 0 Loss: 3.3145 Val Acc: 0.1305\n",
      "Epoch: 100 Loss: 1.7896 Val Acc: 0.3789\n",
      "Epoch: 200 Loss: 1.9786 Val Acc: 0.4021\n",
      "Epoch: 300 Loss: 1.8262 Val Acc: 0.4042\n",
      "Epoch: 400 Loss: 2.1372 Val Acc: 0.3474\n",
      "Epoch: 500 Loss: 1.9378 Val Acc: 0.4021\n",
      "Epoch: 600 Loss: 2.4404 Val Acc: 0.3326\n",
      "Epoch: 700 Loss: 2.8382 Val Acc: 0.1958\n",
      "Epoch: 800 Loss: 2.6916 Val Acc: 0.2168\n",
      "Epoch: 900 Loss: 2.6095 Val Acc: 0.2063\n",
      "Training Time: 42.323596715927124 seconds\n",
      "\n",
      "Training LSTM with sequence length 20\n",
      "Epoch: 0 Loss: 3.2761 Val Acc: 0.1284\n",
      "Epoch: 100 Loss: 1.2795 Val Acc: 0.3979\n",
      "Epoch: 200 Loss: 0.0118 Val Acc: 0.4253\n",
      "Epoch: 300 Loss: 0.0104 Val Acc: 0.4442\n",
      "Epoch: 400 Loss: 0.0099 Val Acc: 0.4484\n",
      "Epoch: 500 Loss: 0.0095 Val Acc: 0.4442\n",
      "Epoch: 600 Loss: 0.0101 Val Acc: 0.4568\n",
      "Epoch: 700 Loss: 0.0094 Val Acc: 0.4442\n",
      "Epoch: 800 Loss: 0.0086 Val Acc: 0.4505\n",
      "Epoch: 900 Loss: 0.0367 Val Acc: 0.4189\n",
      "Training Time: 46.423868894577026 seconds\n",
      "\n",
      "Training GRU with sequence length 20\n",
      "Epoch: 0 Loss: 3.1966 Val Acc: 0.2126\n",
      "Epoch: 100 Loss: 0.0121 Val Acc: 0.4526\n",
      "Epoch: 200 Loss: 0.0163 Val Acc: 0.4505\n",
      "Epoch: 300 Loss: 0.0173 Val Acc: 0.4274\n",
      "Epoch: 400 Loss: 1.0574 Val Acc: 0.2547\n",
      "Epoch: 500 Loss: 0.5953 Val Acc: 0.3116\n",
      "Epoch: 600 Loss: 0.0217 Val Acc: 0.3284\n",
      "Epoch: 700 Loss: 0.0198 Val Acc: 0.2779\n",
      "Epoch: 800 Loss: 0.0186 Val Acc: 0.3158\n",
      "Epoch: 900 Loss: 0.1344 Val Acc: 0.3137\n",
      "Training Time: 43.632535457611084 seconds\n",
      "\n",
      "Training RNN with sequence length 30\n",
      "Epoch: 0 Loss: 3.2579 Val Acc: 0.1987\n",
      "Epoch: 100 Loss: 1.4360 Val Acc: 0.3742\n",
      "Epoch: 200 Loss: 1.8659 Val Acc: 0.3573\n",
      "Epoch: 300 Loss: 1.6546 Val Acc: 0.3805\n",
      "Epoch: 400 Loss: 1.7581 Val Acc: 0.3298\n",
      "Epoch: 500 Loss: 2.2751 Val Acc: 0.2875\n",
      "Epoch: 600 Loss: 2.1376 Val Acc: 0.3171\n",
      "Epoch: 700 Loss: 2.5682 Val Acc: 0.2347\n",
      "Epoch: 800 Loss: 2.5123 Val Acc: 0.2600\n",
      "Epoch: 900 Loss: 2.5015 Val Acc: 0.2558\n",
      "Training Time: 44.75231981277466 seconds\n",
      "\n",
      "Training LSTM with sequence length 30\n",
      "Epoch: 0 Loss: 3.2535 Val Acc: 0.1247\n",
      "Epoch: 100 Loss: 1.0189 Val Acc: 0.4313\n",
      "Epoch: 200 Loss: 0.0264 Val Acc: 0.4397\n",
      "Epoch: 300 Loss: 0.0037 Val Acc: 0.4313\n",
      "Epoch: 400 Loss: 0.0216 Val Acc: 0.4271\n",
      "Epoch: 500 Loss: 0.0026 Val Acc: 0.4186\n",
      "Epoch: 600 Loss: 1.6567 Val Acc: 0.3742\n",
      "Epoch: 700 Loss: 0.0025 Val Acc: 0.4334\n",
      "Epoch: 800 Loss: 0.0024 Val Acc: 0.4461\n",
      "Epoch: 900 Loss: 0.0050 Val Acc: 0.4440\n",
      "Training Time: 46.732426166534424 seconds\n",
      "\n",
      "Training GRU with sequence length 30\n",
      "Epoch: 0 Loss: 3.1476 Val Acc: 0.2262\n",
      "Epoch: 100 Loss: 0.0039 Val Acc: 0.4545\n",
      "Epoch: 200 Loss: 0.0074 Val Acc: 0.4334\n",
      "Epoch: 300 Loss: 1.5546 Val Acc: 0.3848\n",
      "Epoch: 400 Loss: 1.4272 Val Acc: 0.2622\n",
      "Epoch: 500 Loss: 0.4444 Val Acc: 0.2706\n",
      "Epoch: 600 Loss: 0.0324 Val Acc: 0.2791\n",
      "Epoch: 700 Loss: 0.3469 Val Acc: 0.2918\n",
      "Epoch: 800 Loss: 0.0222 Val Acc: 0.3023\n",
      "Epoch: 900 Loss: 0.4528 Val Acc: 0.3171\n",
      "Training Time: 43.30430555343628 seconds\n"
     ]
    }
   ],
   "source": [
    "# Text sequence\n",
    "text = \"\"\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text.\n",
    "\n",
    "At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of text during the training phase of the model.\n",
    "\n",
    "One of the most popular approaches to next character prediction involves the use of Recurrent Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character. LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character prediction tasks.\n",
    "\n",
    "Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time.\n",
    "\n",
    "Once trained, the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and enable more natural interactions with AI-based chatbots and virtual assistants.\n",
    "\n",
    "In summary, next character prediction plays a crucial role in enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future of text-based technology.\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "\n",
    "def encode_text(text):\n",
    "    return [char_to_idx[ch] for ch in text]\n",
    "\n",
    "def one_hot_encode(indices, dict_size):\n",
    "    features = np.zeros((len(indices), dict_size), dtype=np.float32)\n",
    "    for i, idx in enumerate(indices):\n",
    "        features[i, idx] = 1.0\n",
    "    return features\n",
    "\n",
    "# Create input-output pairs for training\n",
    "def create_training_data(text, seq_length):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    for i in range(len(text) - seq_length):\n",
    "        input_seq = text[i:i+seq_length]\n",
    "        target_char = text[i+seq_length]\n",
    "        input_data.append(one_hot_encode(encode_text(input_seq), len(chars)))\n",
    "        target_data.append(char_to_idx[target_char])\n",
    "    return torch.tensor(input_data), torch.tensor(target_data)\n",
    "\n",
    "# Define the model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_type=\"rnn\"):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type == \"rnn\":\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        elif model_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif model_type == \"gru\":\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model_type == \"lstm\":\n",
    "            return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
    "                    torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
    "        else:\n",
    "            return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# Training function\n",
    "def train(model, input_data, target_data, val_input, val_target, epochs, batch_size, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    start_time = time.time()\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, input_data.size(0), batch_size):\n",
    "            batch_input = input_data[i:i+batch_size].to(device)\n",
    "            batch_target = target_data[i:i+batch_size].to(device)\n",
    "            hidden = model.init_hidden(batch_input.size(0))\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(batch_input, hidden)\n",
    "            loss = criterion(output, batch_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / (input_data.size(0) // batch_size)\n",
    "        val_acc = evaluate(model, val_input.to(device), val_target.to(device), batch_size)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model.rnn.__class__.__name__}_best.pth')\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch} Loss: {avg_loss:.4f} Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    print(f'Training Time: {time.time() - start_time} seconds')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, input_data, target_data, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, input_data.size(0), batch_size):\n",
    "            batch_input = input_data[i:i+batch_size]\n",
    "            batch_target = target_data[i:i+batch_size]\n",
    "            hidden = model.init_hidden(batch_input.size(0))\n",
    "            output, hidden = model(batch_input, hidden)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == batch_target).sum().item()\n",
    "    return correct / input_data.size(0)\n",
    "\n",
    "# Main function to run the training\n",
    "def main():\n",
    "    seq_lengths = [10, 20, 30]\n",
    "    hidden_size = 128\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "    models = [\"rnn\", \"lstm\", \"gru\"]\n",
    "\n",
    "    for seq_length in seq_lengths:\n",
    "        input_data, target_data = create_training_data(text, seq_length)\n",
    "        train_input, val_input, train_target, val_target = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "        for model_type in models:\n",
    "            print(f'\\nTraining {model_type.upper()} with sequence length {seq_length}')\n",
    "            model = CharRNN(len(chars), hidden_size, len(chars), model_type=model_type).to(device)\n",
    "            train(model, train_input, train_target, val_input, val_target, epochs, batch_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN with sequence length 10\n",
      "Model Size (Number of Parameters): 28205\n",
      "Epoch: 0 Loss: 3.4678 Val Acc: 0.1174\n",
      "Epoch: 100 Loss: 1.3404 Val Acc: 0.3669\n",
      "Epoch: 200 Loss: 1.5257 Val Acc: 0.4046\n",
      "Epoch: 300 Loss: 1.3716 Val Acc: 0.3983\n",
      "Epoch: 400 Loss: 1.0262 Val Acc: 0.4423\n",
      "Epoch: 500 Loss: 2.1423 Val Acc: 0.3522\n",
      "Epoch: 600 Loss: 1.7440 Val Acc: 0.3816\n",
      "Epoch: 700 Loss: 1.7582 Val Acc: 0.4151\n",
      "Epoch: 800 Loss: 1.5405 Val Acc: 0.3983\n",
      "Epoch: 900 Loss: 2.1467 Val Acc: 0.3396\n",
      "Training Time: 42.70583724975586 seconds\n",
      "\n",
      "Training LSTM with sequence length 10\n",
      "Model Size (Number of Parameters): 95405\n",
      "Epoch: 0 Loss: 3.2891 Val Acc: 0.1426\n",
      "Epoch: 100 Loss: 0.0381 Val Acc: 0.4423\n",
      "Epoch: 200 Loss: 0.0377 Val Acc: 0.4298\n",
      "Epoch: 300 Loss: 0.0446 Val Acc: 0.4675\n",
      "Epoch: 400 Loss: 0.0341 Val Acc: 0.4486\n",
      "Epoch: 500 Loss: 0.0346 Val Acc: 0.4864\n",
      "Epoch: 600 Loss: 0.0399 Val Acc: 0.4885\n",
      "Epoch: 700 Loss: 0.0332 Val Acc: 0.4864\n",
      "Epoch: 800 Loss: 0.0322 Val Acc: 0.4822\n",
      "Epoch: 900 Loss: 0.0336 Val Acc: 0.4864\n",
      "Training Time: 44.80360794067383 seconds\n",
      "\n",
      "Training GRU with sequence length 10\n",
      "Model Size (Number of Parameters): 73005\n",
      "Epoch: 0 Loss: 3.2096 Val Acc: 0.2432\n",
      "Epoch: 100 Loss: 0.0475 Val Acc: 0.4423\n",
      "Epoch: 200 Loss: 0.0510 Val Acc: 0.4549\n",
      "Epoch: 300 Loss: 0.0590 Val Acc: 0.4025\n",
      "Epoch: 400 Loss: 0.0700 Val Acc: 0.4172\n",
      "Epoch: 500 Loss: 0.0854 Val Acc: 0.4235\n",
      "Epoch: 600 Loss: 0.7767 Val Acc: 0.4193\n",
      "Epoch: 700 Loss: 0.0818 Val Acc: 0.4025\n",
      "Epoch: 800 Loss: 1.5901 Val Acc: 0.3312\n",
      "Epoch: 900 Loss: 0.0611 Val Acc: 0.4004\n",
      "Training Time: 41.08431100845337 seconds\n",
      "\n",
      "Training RNN with sequence length 20\n",
      "Model Size (Number of Parameters): 28205\n",
      "Epoch: 0 Loss: 3.3059 Val Acc: 0.2021\n",
      "Epoch: 100 Loss: 1.7206 Val Acc: 0.3347\n",
      "Epoch: 200 Loss: 1.7609 Val Acc: 0.3705\n",
      "Epoch: 300 Loss: 2.2185 Val Acc: 0.3284\n",
      "Epoch: 400 Loss: 2.0644 Val Acc: 0.3474\n",
      "Epoch: 500 Loss: 1.8566 Val Acc: 0.3579\n",
      "Epoch: 600 Loss: 1.8731 Val Acc: 0.3537\n",
      "Epoch: 700 Loss: 1.8893 Val Acc: 0.3747\n",
      "Epoch: 800 Loss: 2.0050 Val Acc: 0.3474\n",
      "Epoch: 900 Loss: 2.3158 Val Acc: 0.3095\n",
      "Training Time: 40.93763756752014 seconds\n",
      "\n",
      "Training LSTM with sequence length 20\n",
      "Model Size (Number of Parameters): 95405\n",
      "Epoch: 0 Loss: 3.2682 Val Acc: 0.1284\n",
      "Epoch: 100 Loss: 0.0099 Val Acc: 0.4463\n",
      "Epoch: 200 Loss: 1.3066 Val Acc: 0.4463\n",
      "Epoch: 300 Loss: 0.0113 Val Acc: 0.4379\n",
      "Epoch: 400 Loss: 0.0114 Val Acc: 0.4526\n",
      "Epoch: 500 Loss: 0.0117 Val Acc: 0.4526\n",
      "Epoch: 600 Loss: 0.0095 Val Acc: 0.4547\n",
      "Epoch: 700 Loss: 0.0099 Val Acc: 0.4126\n",
      "Epoch: 800 Loss: 0.0097 Val Acc: 0.4400\n",
      "Epoch: 900 Loss: 0.0146 Val Acc: 0.4379\n",
      "Training Time: 44.73575210571289 seconds\n",
      "\n",
      "Training GRU with sequence length 20\n",
      "Model Size (Number of Parameters): 73005\n",
      "Epoch: 0 Loss: 3.1880 Val Acc: 0.2042\n",
      "Epoch: 100 Loss: 0.0124 Val Acc: 0.4547\n",
      "Epoch: 200 Loss: 0.0152 Val Acc: 0.4526\n",
      "Epoch: 300 Loss: 0.3721 Val Acc: 0.4021\n",
      "Epoch: 400 Loss: 0.0166 Val Acc: 0.4021\n",
      "Epoch: 500 Loss: 0.0244 Val Acc: 0.4084\n",
      "Epoch: 600 Loss: 0.0231 Val Acc: 0.3874\n",
      "Epoch: 700 Loss: 0.0399 Val Acc: 0.4042\n",
      "Epoch: 800 Loss: 0.1656 Val Acc: 0.3958\n",
      "Epoch: 900 Loss: 0.0174 Val Acc: 0.3705\n",
      "Training Time: 45.570786237716675 seconds\n",
      "\n",
      "Training RNN with sequence length 30\n",
      "Model Size (Number of Parameters): 28205\n",
      "Epoch: 0 Loss: 3.2595 Val Acc: 0.1649\n",
      "Epoch: 100 Loss: 1.7846 Val Acc: 0.3531\n",
      "Epoch: 200 Loss: 2.3831 Val Acc: 0.2706\n",
      "Epoch: 300 Loss: 2.2409 Val Acc: 0.2939\n",
      "Epoch: 400 Loss: 2.1396 Val Acc: 0.3467\n",
      "Epoch: 500 Loss: 2.2782 Val Acc: 0.2791\n",
      "Epoch: 600 Loss: 2.1245 Val Acc: 0.2875\n",
      "Epoch: 700 Loss: 2.4642 Val Acc: 0.2748\n",
      "Epoch: 800 Loss: 2.4203 Val Acc: 0.2474\n",
      "Epoch: 900 Loss: 2.6162 Val Acc: 0.2283\n",
      "Training Time: 44.59588646888733 seconds\n",
      "\n",
      "Training LSTM with sequence length 30\n",
      "Model Size (Number of Parameters): 95405\n",
      "Epoch: 0 Loss: 3.2635 Val Acc: 0.1268\n",
      "Epoch: 100 Loss: 0.0031 Val Acc: 0.4397\n",
      "Epoch: 200 Loss: 0.0147 Val Acc: 0.4144\n",
      "Epoch: 300 Loss: 0.0057 Val Acc: 0.4207\n",
      "Epoch: 400 Loss: 0.0092 Val Acc: 0.4228\n",
      "Epoch: 500 Loss: 0.0456 Val Acc: 0.4440\n",
      "Epoch: 600 Loss: 0.0043 Val Acc: 0.4017\n",
      "Epoch: 700 Loss: 0.0033 Val Acc: 0.4292\n",
      "Epoch: 800 Loss: 0.0026 Val Acc: 0.4376\n",
      "Epoch: 900 Loss: 0.6236 Val Acc: 0.3890\n",
      "Training Time: 48.176032304763794 seconds\n",
      "\n",
      "Training GRU with sequence length 30\n",
      "Model Size (Number of Parameters): 73005\n",
      "Epoch: 0 Loss: 3.2082 Val Acc: 0.2368\n",
      "Epoch: 100 Loss: 0.0035 Val Acc: 0.4397\n",
      "Epoch: 200 Loss: 0.0062 Val Acc: 0.4292\n",
      "Epoch: 300 Loss: 1.5795 Val Acc: 0.3235\n",
      "Epoch: 400 Loss: 1.3693 Val Acc: 0.3319\n",
      "Epoch: 500 Loss: 1.0827 Val Acc: 0.3044\n",
      "Epoch: 600 Loss: 0.5000 Val Acc: 0.3383\n",
      "Epoch: 700 Loss: 0.0169 Val Acc: 0.3404\n",
      "Epoch: 800 Loss: 0.0786 Val Acc: 0.3552\n",
      "Epoch: 900 Loss: 0.0151 Val Acc: 0.3784\n",
      "Training Time: 46.73784255981445 seconds\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Main function to run the training\n",
    "def main():\n",
    "    seq_lengths = [10, 20, 30]\n",
    "    hidden_size = 128\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "    models = [\"rnn\", \"lstm\", \"gru\"]\n",
    "\n",
    "    for seq_length in seq_lengths:\n",
    "        input_data, target_data = create_training_data(text, seq_length)\n",
    "        train_input, val_input, train_target, val_target = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "        for model_type in models:\n",
    "            print(f'\\nTraining {model_type.upper()} with sequence length {seq_length}')\n",
    "            model = CharRNN(len(chars), hidden_size, len(chars), model_type=model_type).to(device)\n",
    "            print(f'Model Size (Number of Parameters): {count_parameters(model)}')\n",
    "            train(model, train_input, train_target, val_input, val_target, epochs, batch_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
